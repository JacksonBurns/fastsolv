{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobolev Demo\n",
    "\n",
    "This notebook demonstrates how to setup sobolev training for a single molecule property prediction task.\n",
    "The actual `fastsolv` model takes two molecules at a time, but the core idea of Sobolev training is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Your data should come in a format that looks something like what is shown below that can be loaded into a `pandas.DataFrame`.\n",
    "For this example, we are calculating the gas phase heat capacity for various small molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = '''\n",
    "Name of compounds,Formula,SMILES,CAS,Cp (J/mol.K),T (K)\n",
    "Formaldehyde,CH2O,C=O,50-00-0,33.5,200.0\n",
    "Formaldehyde,CH2O,C=O,50-00-0,34.7,273.15\n",
    "Formaldehyde,CH2O,C=O,50-00-0,35.44,300.0\n",
    "Formaldehyde,CH2O,C=O,50-00-0,39.24,400.0\n",
    "Formic acid,CH2O2,O=CO,64-18-6,37.83,200.0\n",
    "Formic acid,CH2O2,O=CO,64-18-6,43.54,273.15\n",
    "Formic acid,CH2O2,O=CO,64-18-6,45.84,300.0\n",
    "Formic acid,CH2O2,O=CO,64-18-6,54.52,400.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of compounds</th>\n",
       "      <th>Formula</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>CAS</th>\n",
       "      <th>Cp (J/mol.K)</th>\n",
       "      <th>T (K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Formaldehyde</td>\n",
       "      <td>CH2O</td>\n",
       "      <td>C=O</td>\n",
       "      <td>50-00-0</td>\n",
       "      <td>33.50</td>\n",
       "      <td>200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Formaldehyde</td>\n",
       "      <td>CH2O</td>\n",
       "      <td>C=O</td>\n",
       "      <td>50-00-0</td>\n",
       "      <td>34.70</td>\n",
       "      <td>273.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Formaldehyde</td>\n",
       "      <td>CH2O</td>\n",
       "      <td>C=O</td>\n",
       "      <td>50-00-0</td>\n",
       "      <td>35.44</td>\n",
       "      <td>300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Formaldehyde</td>\n",
       "      <td>CH2O</td>\n",
       "      <td>C=O</td>\n",
       "      <td>50-00-0</td>\n",
       "      <td>39.24</td>\n",
       "      <td>400.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Formic acid</td>\n",
       "      <td>CH2O2</td>\n",
       "      <td>O=CO</td>\n",
       "      <td>64-18-6</td>\n",
       "      <td>37.83</td>\n",
       "      <td>200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Formic acid</td>\n",
       "      <td>CH2O2</td>\n",
       "      <td>O=CO</td>\n",
       "      <td>64-18-6</td>\n",
       "      <td>43.54</td>\n",
       "      <td>273.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Formic acid</td>\n",
       "      <td>CH2O2</td>\n",
       "      <td>O=CO</td>\n",
       "      <td>64-18-6</td>\n",
       "      <td>45.84</td>\n",
       "      <td>300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Formic acid</td>\n",
       "      <td>CH2O2</td>\n",
       "      <td>O=CO</td>\n",
       "      <td>64-18-6</td>\n",
       "      <td>54.52</td>\n",
       "      <td>400.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name of compounds Formula SMILES      CAS  Cp (J/mol.K)   T (K)\n",
       "0      Formaldehyde    CH2O    C=O  50-00-0         33.50  200.00\n",
       "1      Formaldehyde    CH2O    C=O  50-00-0         34.70  273.15\n",
       "2      Formaldehyde    CH2O    C=O  50-00-0         35.44  300.00\n",
       "3      Formaldehyde    CH2O    C=O  50-00-0         39.24  400.00\n",
       "4       Formic acid   CH2O2   O=CO  64-18-6         37.83  200.00\n",
       "5       Formic acid   CH2O2   O=CO  64-18-6         43.54  273.15\n",
       "6       Formic acid   CH2O2   O=CO  64-18-6         45.84  300.00\n",
       "7       Formic acid   CH2O2   O=CO  64-18-6         54.52  400.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(StringIO(data_csv))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can calculate the features needed for our models during training.\n",
    "This is adapted from the code in `data/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"SMILES\":\"smiles\", \"Cp (J/mol.K)\" :\"target\", \"T (K)\":\"independent_variable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastprop.defaults import ALL_2D\n",
    "from fastprop.descriptors import get_descriptors\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "def get_descs(src_df: pd.DataFrame):\n",
    "    \"\"\"Calculates features for molecules\n",
    "\n",
    "    Args:\n",
    "        src_df (pd.DataFrame): DataFrame with 'smiles', 'target', 'independent_variable'.\n",
    "                               Other columns will be ignored.\n",
    "    \"\"\"\n",
    "    unique_smiles: np.ndarray = pd.unique(src_df[\"smiles\"])\n",
    "    descs: np.ndarray = get_descriptors(False, ALL_2D, list(Chem.MolFromSmiles(i) for i in unique_smiles)).to_numpy(dtype=np.float32)\n",
    "    # assemble the data into the format expected in fastprop\n",
    "    # map smiles -> descriptors\n",
    "    smiles_to_descs: dict = {smiles: desc for smiles, desc in zip(unique_smiles, descs)}\n",
    "    fastprop_data: pd.DataFrame = src_df[[\"smiles\", \"target\", \"independent_variable\"]]\n",
    "    fastprop_data: pd.DataFrame = fastprop_data.reindex(columns=fastprop_data.columns.tolist() + ALL_2D)\n",
    "    fastprop_data[ALL_2D] = [smiles_to_descs[smi] for smi in fastprop_data[\"smiles\"]]\n",
    "    return fastprop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 41.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>target</th>\n",
       "      <th>independent_variable</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C=O</td>\n",
       "      <td>33.50</td>\n",
       "      <td>200.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.493062</td>\n",
       "      <td>30.010565</td>\n",
       "      <td>7.502641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C=O</td>\n",
       "      <td>34.70</td>\n",
       "      <td>273.15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.493062</td>\n",
       "      <td>30.010565</td>\n",
       "      <td>7.502641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C=O</td>\n",
       "      <td>35.44</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.493062</td>\n",
       "      <td>30.010565</td>\n",
       "      <td>7.502641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C=O</td>\n",
       "      <td>39.24</td>\n",
       "      <td>400.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>7.493062</td>\n",
       "      <td>30.010565</td>\n",
       "      <td>7.502641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=CO</td>\n",
       "      <td>37.83</td>\n",
       "      <td>200.00</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>...</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>17.310770</td>\n",
       "      <td>46.005478</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O=CO</td>\n",
       "      <td>43.54</td>\n",
       "      <td>273.15</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>...</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>17.310770</td>\n",
       "      <td>46.005478</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O=CO</td>\n",
       "      <td>45.84</td>\n",
       "      <td>300.00</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>...</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>17.310770</td>\n",
       "      <td>46.005478</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O=CO</td>\n",
       "      <td>54.52</td>\n",
       "      <td>400.00</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>...</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>17.310770</td>\n",
       "      <td>46.005478</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  smiles  target  independent_variable       ABC     ABCGG  nAcid  nBase  \\\n",
       "0    C=O   33.50                200.00  0.000000  0.000000    0.0    0.0   \n",
       "1    C=O   34.70                273.15  0.000000  0.000000    0.0    0.0   \n",
       "2    C=O   35.44                300.00  0.000000  0.000000    0.0    0.0   \n",
       "3    C=O   39.24                400.00  0.000000  0.000000    0.0    0.0   \n",
       "4   O=CO   37.83                200.00  1.414214  1.414214    1.0    0.0   \n",
       "5   O=CO   43.54                273.15  1.414214  1.414214    1.0    0.0   \n",
       "6   O=CO   45.84                300.00  1.414214  1.414214    1.0    0.0   \n",
       "7   O=CO   54.52                400.00  1.414214  1.414214    1.0    0.0   \n",
       "\n",
       "    SpAbs_A   SpMax_A  SpDiam_A  ...     SRW10     TSRW10         MW  \\\n",
       "0  2.000000  1.000000  2.000000  ...  1.098612   7.493062  30.010565   \n",
       "1  2.000000  1.000000  2.000000  ...  1.098612   7.493062  30.010565   \n",
       "2  2.000000  1.000000  2.000000  ...  1.098612   7.493062  30.010565   \n",
       "3  2.000000  1.000000  2.000000  ...  1.098612   7.493062  30.010565   \n",
       "4  2.828427  1.414214  2.828427  ...  4.174387  17.310770  46.005478   \n",
       "5  2.828427  1.414214  2.828427  ...  4.174387  17.310770  46.005478   \n",
       "6  2.828427  1.414214  2.828427  ...  4.174387  17.310770  46.005478   \n",
       "7  2.828427  1.414214  2.828427  ...  4.174387  17.310770  46.005478   \n",
       "\n",
       "        AMW  WPath  WPol  Zagreb1  Zagreb2  mZagreb1  mZagreb2  \n",
       "0  7.502641    1.0   0.0      2.0      1.0      2.00       1.0  \n",
       "1  7.502641    1.0   0.0      2.0      1.0      2.00       1.0  \n",
       "2  7.502641    1.0   0.0      2.0      1.0      2.00       1.0  \n",
       "3  7.502641    1.0   0.0      2.0      1.0      2.00       1.0  \n",
       "4  9.201096    4.0   0.0      6.0      4.0      2.25       1.0  \n",
       "5  9.201096    4.0   0.0      6.0      4.0      2.25       1.0  \n",
       "6  9.201096    4.0   0.0      6.0      4.0      2.25       1.0  \n",
       "7  9.201096    4.0   0.0      6.0      4.0      2.25       1.0  \n",
       "\n",
       "[8 rows x 1616 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data: pd.DataFrame = get_descs(df)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the gradients of our target property with respect to the independent variable.\n",
    "To do so, we will first write a helper function that calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _f(r):\n",
    "    if len(r[\"scaled_target\"]) == 1:\n",
    "        return [np.nan]\n",
    "    sorted_idxs = np.argsort(r[\"scaled_independent_variable\"])\n",
    "    unsort_idxs = np.argsort(sorted_idxs)\n",
    "    # mask out enormous nan/inf\n",
    "    grads = [\n",
    "        i if np.isfinite(i) else np.nan\n",
    "        for i in np.gradient(\n",
    "            [r[\"scaled_target\"][i] for i in sorted_idxs],\n",
    "            [r[\"scaled_independent_variable\"][i] for i in sorted_idxs],\n",
    "        )\n",
    "    ]\n",
    "    return [grads[i] for i in unsort_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically you would need to decide which points to use for validation and which for training - for this simple demo, we will train without a validation set.\n",
    "Keep in mind that because we are using gradients, you must include all rows of your data for a given molecule at every temperature in either training _or_ validation to avoid leaking data.\n",
    "For example, in this case we would not want to included formaldehyde at 200 K in training and then 273 K in validation.\n",
    "This is because when we calculate the gradient at 200 K we use information about the point at 273 K, which would leak data if they are not kept together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by re-scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = StandardScaler().fit(data[[\"target\"]])\n",
    "scaled_target = target_scaler.transform(data[[\"target\"]]).ravel()\n",
    "independent_variable_scaler = StandardScaler().fit(data[[\"independent_variable\"]])\n",
    "scaled_independent_variable = independent_variable_scaler.transform(df[[\"independent_variable\"]]).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we just use pandas to calculate the gradients - the inline comments provide greater detail on how it actually works (it's rather involved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grads = pd.concat(\n",
    "    (\n",
    "        df,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"source_index\": np.arange(len(df[\"independent_variable\"])),\n",
    "                \"scaled_independent_variable\": scaled_independent_variable,\n",
    "                \"scaled_target\": scaled_target,\n",
    "            }\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "# group the data by molecule\n",
    "grads = grads.groupby([\"smiles\"])[[\"scaled_target\", \"scaled_independent_variable\", \"source_index\"]].aggregate(list)\n",
    "# calculate the gradient at each measurement of target wrt independent_variable\n",
    "grads[\"grad\"] = grads.apply(_f, axis=1)\n",
    "# get them in the same order as the source data\n",
    "grads = grads.explode([\"grad\", \"source_index\"]).sort_values(by=\"source_index\")\n",
    "# convert and mask\n",
    "grads = grads[\"grad\"].to_numpy(dtype=np.float32)\n",
    "_mask = np.isnan(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready!\n",
    "We can re-use this same input file for both a `fastprop`- and `chemprop`-based model.\n",
    "For large, real-world datasets, it is highly suggested to save the features to a file (like a `.csv` with `df.to_csv`) to allow re-loading it without recalculating in the future.\n",
    "\n",
    "Later on in this notebook we will actually pack these values into data classes that `torch` knows how to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Models\n",
    "\n",
    "The model definitions for the original study are in `models/fastprop` and `models/chemprop`.\n",
    "Each expects two molecules, so we will adapt that code here to work for our one molecule case.\n",
    "\n",
    "For the sake of demonstration, the below variable can be changed to enable or disable the use of Sobolev loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_SOBOLEV_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fastprop`-based Model\n",
    "\n",
    "First, we'll define the model code for a `fastprop`-based model.\n",
    "Most of the model functionality is handled by the `fastprop` model definition - you can find the source for `fastprop` [here](https://github.com/JacksonBurns/fastprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprop.model import fastprop as _fastprop\n",
    "from fastprop.data import standard_scale, inverse_standard_scale\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concatenation class will just be used to combine the input temperature with the molecule features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concatenation(torch.nn.Module):\n",
    "    def forward(self, batch):\n",
    "        return torch.cat(batch, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fastpropSobolev(_fastprop):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: 2,\n",
    "        hidden_size: 1_800,\n",
    "        num_features: int = 1613,\n",
    "        learning_rate: float = 0.0001,\n",
    "        target_means: torch.Tensor = None,\n",
    "        target_vars: torch.Tensor = None,\n",
    "        feature_means: torch.Tensor = None,\n",
    "        feature_vars: torch.Tensor = None,\n",
    "        independent_variable_means: torch.Tensor = None,\n",
    "        independent_variable_vars: torch.Tensor = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            input_size=num_features + 1,\n",
    "            hidden_size=hidden_size,\n",
    "            fnn_layers=num_layers,\n",
    "            readout_size=1,\n",
    "            num_tasks=1,\n",
    "            learning_rate=learning_rate,\n",
    "            problem_type=\"regression\",\n",
    "            target_names=[],\n",
    "            target_means=target_means,\n",
    "            target_vars=target_vars,\n",
    "            feature_means=feature_means,\n",
    "            feature_vars=feature_vars,\n",
    "            clamp_input=True,\n",
    "        )\n",
    "\n",
    "        # for later predicting\n",
    "        self.register_buffer(\"independent_variable_means\", independent_variable_means)\n",
    "        self.register_buffer(\"independent_variable_vars\", independent_variable_vars)\n",
    "\n",
    "        # add concatenation of the temperature to the input features\n",
    "        _modules = OrderedDict()\n",
    "        _modules['concatenate'] = Concatenation()\n",
    "        for name, module in self.fnn.named_children():\n",
    "            _modules[name] = module\n",
    "        self.fnn = torch.nn.Sequential(_modules)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        err_msg = \"\"\n",
    "        for stat_obj, stat_name in zip(\n",
    "            (\n",
    "                self.feature_means,\n",
    "                self.feature_vars,\n",
    "                self.independent_variable_means,\n",
    "                self.independent_variable_vars,\n",
    "                self.target_means,\n",
    "                self.target_vars,\n",
    "            ),\n",
    "            (\n",
    "                \"feature_means\",\n",
    "                \"feature_vars\",\n",
    "                \"independent_variable_means\",\n",
    "                \"independent_variable_vars\",\n",
    "                \"target_means\",\n",
    "                \"target_vars\",\n",
    "            ),\n",
    "        ):\n",
    "            if stat_obj is None:\n",
    "                err_msg.append(f\"{stat_name} is None!\\n\")\n",
    "        if err_msg:\n",
    "            raise RuntimeError(\"Missing scaler statistics!\\n\" + err_msg)\n",
    "\n",
    "        features, independent_variables = batch[0]  # batch 1 is solubility\n",
    "        features = standard_scale(features, self.feature_means, self.feature_vars)\n",
    "        independent_variables = standard_scale(independent_variables, self.independent_variable_means, self.independent_variable_vars)\n",
    "        with torch.inference_mode():\n",
    "            logits = self.forward((features, independent_variables))\n",
    "        return inverse_standard_scale(logits, self.target_means, self.target_vars)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def _custom_loss(self, batch: tuple[tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor], name: str):\n",
    "        (_features, independent_variable), y, y_grad = batch\n",
    "        independent_variable.requires_grad_()\n",
    "        y_hat: torch.Tensor = self.forward((_features, independent_variable))\n",
    "        y_loss = torch.nn.functional.mse_loss(y_hat, y, reduction=\"mean\")\n",
    "        (y_grad_hat,) = torch.autograd.grad(\n",
    "            y_hat,\n",
    "            independent_variable,\n",
    "            grad_outputs=torch.ones_like(y_hat),\n",
    "            retain_graph=True,\n",
    "        )\n",
    "        _scale_factor = 10.0\n",
    "        y_grad_loss = _scale_factor * (y_grad_hat - y_grad).pow(2).nanmean()  # MSE ignoring nan\n",
    "        loss = y_loss + y_grad_loss\n",
    "        self.log(f\"{name}/{self.training_metric}_scaled_loss\", loss)\n",
    "        self.log(f\"{name}/logS_scaled_loss\", y_loss)\n",
    "        self.log(f\"{name}/dlogSdT_scaled_loss\", y_grad_loss)\n",
    "        return loss, y_hat\n",
    "\n",
    "    def _plain_loss(self, batch: tuple[tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor], name: str):\n",
    "        (_features, independent_variable), y, _ = batch\n",
    "        y_hat: torch.Tensor = self.forward((_features, independent_variable))\n",
    "        loss = torch.nn.functional.mse_loss(y_hat, y, reduction=\"mean\")\n",
    "        self.log(f\"{name}/{self.training_metric}_scaled_loss\", loss)\n",
    "        return loss, y_hat\n",
    "\n",
    "    def _loss(self, batch: tuple[tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor], name: str):\n",
    "        if ENABLE_SOBOLEV_LOSS:\n",
    "            return self._plain_loss(batch, name)\n",
    "        else:\n",
    "            return self._custom_loss(batch, name)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._loss(batch, \"train\")[0]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat = self._loss(batch, \"validation\")\n",
    "        self._human_loss(y_hat, batch, \"validation\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y_hat = self._loss(batch, \"test\")\n",
    "        self._human_loss(y_hat, batch, \"test\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `chemprop`-based Model\n",
    "\n",
    "The one 'trick' for the `chemprop`-derived model is the use of a custom metric.\n",
    "This code specifically works for `chemprop` version 2.0 - it would need some (small) changes to work with 2.1 or newer, which changed the way that metrics work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop import models\n",
    "from chemprop.nn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSEMetric(metrics.MSEMetric):\n",
    "    def forward(self, preds, targets, mask, weights, lt_mask, gt_mask):\n",
    "        return torch.nn.functional.mse_loss(preds, targets[:, 0, None], reduction=\"mean\")\n",
    "\n",
    "\n",
    "class SobolevMPNN(models.MPNN):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._sobolev_loss(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._sobolev_loss(batch, \"val\")\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def _sobolev_loss(self, batch, name):\n",
    "        bmg, V_d, X_d, targets, *_ = batch\n",
    "        # track grad for temperature\n",
    "        X_d.requires_grad_()\n",
    "        Z = self.fingerprint(bmg, V_d, X_d)\n",
    "        y_hat = self.predictor.train_step(Z)\n",
    "        y_loss = torch.nn.functional.mse_loss(y_hat, targets[:, 0, None], reduction=\"mean\")\n",
    "        (y_grad_hat,) = torch.autograd.grad(\n",
    "            y_hat,\n",
    "            X_d,\n",
    "            grad_outputs=torch.ones_like(y_hat),\n",
    "            retain_graph=True,\n",
    "        )\n",
    "        _scale_factor = 1.0\n",
    "        y_grad_loss = _scale_factor * (y_grad_hat - targets[:, 1]).pow(2).nanmean()  # MSE ignoring nan\n",
    "        loss = y_loss + y_grad_loss\n",
    "        self.log(f\"{name}/sobolev_loss\", loss, batch_size=len(batch[0]))\n",
    "        self.log(f\"{name}/logs_loss\", y_loss, batch_size=len(batch[0]))\n",
    "        self.log(f\"{name}/grad_loss\", y_grad_loss, batch_size=len(batch[0]))\n",
    "        self.log(f\"{name}_loss\", loss, prog_bar=True, batch_size=len(batch[0]))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with `torch`\n",
    "\n",
    "Now that we have our data and models we can start setting up our code to interact with `torch`.\n",
    "\n",
    "### `fastprop` Data\n",
    "\n",
    "We'll start by defining a new dataset class to help interoperate our data with `fastprop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolubilityDataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        independent_variable: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        target_gradient: torch.Tensor,\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.independent_variable = independent_variable\n",
    "        self.target = target\n",
    "        self.target_gradient = target_gradient\n",
    "        self.length = features.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            (\n",
    "                self.features[index],\n",
    "                self.independent_variable[index],\n",
    "            ),\n",
    "            self.target[index],\n",
    "            self.target_gradient[index],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the input feature for fastprop (we already rescaled the target and independent variable).\n",
    "If we had a validation set, we would want to be very careful to apply the scaler fit on just the _training_ data to the _validation_ data, again avoiding data leaks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, feature_means, feature_vars = standard_scale(torch.tensor(data[ALL_2D].to_numpy(dtype=np.float32)))\n",
    "tens = lambda d: torch.tensor(d, dtype=torch.float32)\n",
    "target_means, target_vars = tens(target_scaler.mean_), tens(target_scaler.var_)\n",
    "independent_variable_means, independent_variable_vars = tens(independent_variable_scaler.mean_), tens(independent_variable_scaler.var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `[:, None]` syntax to insert a second dimension all of our 1D tensors - this makes concatenating things easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastprop_dataset = SolubilityDataset(\n",
    "    features=features,\n",
    "    independent_variable=tens(scaled_independent_variable)[:, None],\n",
    "    target=tens(scaled_target)[:, None],\n",
    "    target_gradient=tens(grads)[:, None],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `chemprop` Data\n",
    "\n",
    "We can use the existing `chemprop` classes right out-of-the-box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.data import MoleculeDatapoint, MoleculeDataset\n",
    "from chemprop.featurizers import SimpleMoleculeMolGraphFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackson/miniconda3/envs/fastsolv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chemprop_data = [\n",
    "    MoleculeDatapoint.from_smi(smi, [tgt, grd], x_d=np.array([ind]))\n",
    "    for smi, tgt, grd, ind in zip(data[\"smiles\"], scaled_target, grads, data[\"independent_variable\"])\n",
    "]\n",
    "featurizer = SimpleMoleculeMolGraphFeaturizer()\n",
    "chemprop_dataset = MoleculeDataset(chemprop_data, featurizer)\n",
    "chemprop_dataset.normalize_inputs(\"X_d\", independent_variable_scaler)\n",
    "chemprop_dataset.cache = True\n",
    "# if we had a validation set, we would want to set it up here.\n",
    "# chemprop 2.0 docs say to do this, but it actually gets scaled during training\n",
    "# this is fixed in chemprop 2.1 (but we wrote this code in 2.0) so stick to 2.0 for now\n",
    "# val_dataset = MoleculeDataset(chemprop_validation_data, featurizer)\n",
    "# val_dataset.normalize_inputs(\"X_d\", independent_variable_sccaler)\n",
    "# val_dataset.cache = True       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we can finally train the model!\n",
    "This is just the typical pytorch lightning training setup from here on out.\n",
    "This notebook shows how to run both the `fastprop`- and `chemprop`-based model.\n",
    "\n",
    "The last small bit of difficulty is that `chemprop` and `fastprop` need a slightly different setup for `Trainer` and other imports because `lightning` changed their syntax in between the time period when the two were developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from chemprop.data.dataloader import build_dataloader\n",
    "from chemprop import nn\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint as NewModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger as NewTensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint as OldModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger as OldTensorBoardLogger\n",
    "\n",
    "# from lightning.pytorch.callbacks.early_stopping import EarlyStopping  <-- uncomment if you have a validation set for early stopping (recommended!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastprop_dataloader = DataLoader(fastprop_dataset, batch_size=4)\n",
    "chemprop_loader = build_dataloader(chemprop_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastpropSobolev(\n",
       "  (fnn): Sequential(\n",
       "    (concatenate): Concatenation()\n",
       "    (clamp): ClampN(n=3)\n",
       "    (lin1): Linear(in_features=1614, out_features=400, bias=True)\n",
       "    (act1): ReLU()\n",
       "    (lin2): Linear(in_features=400, out_features=400, bias=True)\n",
       "  )\n",
       "  (readout): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastprop_model = fastpropSobolev(\n",
    "    num_layers=2,\n",
    "    hidden_size=400,\n",
    "    num_features=len(ALL_2D),\n",
    "    target_means=target_means,\n",
    "    target_vars=target_vars,\n",
    "    feature_means=feature_means,\n",
    "    feature_vars=feature_vars,\n",
    "    independent_variable_means=independent_variable_means,\n",
    "    independent_variable_vars=independent_variable_vars,\n",
    ")\n",
    "fastprop_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SobolevMPNN(\n",
       "  (message_passing): BondMessagePassing(\n",
       "    (W_i): Linear(in_features=86, out_features=300, bias=False)\n",
       "    (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (W_o): Linear(in_features=372, out_features=300, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (tau): ReLU()\n",
       "    (V_d_transform): Identity()\n",
       "    (graph_transform): Identity()\n",
       "  )\n",
       "  (agg): MeanAggregation()\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (predictor): RegressionFFN(\n",
       "    (ffn): MLP(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=301, out_features=300, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=300, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (criterion): CustomMSEMetric(task_weights=[[1.0]])\n",
       "    (output_transform): UnscaleTransform()\n",
       "  )\n",
       "  (X_d_transform): ScaleTransform()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = nn.BondMessagePassing()\n",
    "agg = nn.MeanAggregation()\n",
    "output_transform = nn.UnscaleTransform.from_standard_scaler(target_scaler)\n",
    "ffn = nn.RegressionFFN(\n",
    "    input_dim=mp.output_dim + 1,  # temperature\n",
    "    n_layers=2,\n",
    "    criterion=CustomMSEMetric(),\n",
    "    output_transform=output_transform,\n",
    ")\n",
    "X_d_transform = nn.ScaleTransform.from_standard_scaler(independent_variable_scaler)\n",
    "metric_list = [CustomMSEMetric()]\n",
    "chemprop_model = SobolevMPNN(\n",
    "    mp,\n",
    "    agg,\n",
    "    ffn,\n",
    "    batch_norm=True,\n",
    "    metrics=metric_list,\n",
    "    X_d_transform=X_d_transform,\n",
    ")\n",
    "chemprop_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we would want to use early stopping - for this demo notebook we don't have a validation set, but the code needed to implement early stopping is shown in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_outdir = Path(\"demo_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the actual training call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer as NewTrainer\n",
    "from pytorch_lightning import Trainer as OldTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jackson/miniconda3/envs/fastsolv/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | fnn     | Sequential | 806 K  | train\n",
      "1 | readout | Linear     | 401    | train\n",
      "-----------------------------------------------\n",
      "806 K     Trainable params\n",
      "0         Non-trainable params\n",
      "806 K     Total params\n",
      "3.227     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 16.28it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 13.64it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "tensorboard_logger = OldTensorBoardLogger(\n",
    "    _outdir,\n",
    "    name=\"tensorboard_logs\",\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "callbacks = [\n",
    "    # EarlyStopping(\n",
    "    #     monitor=\"val_loss\",\n",
    "    #     mode=\"min\",\n",
    "    #     verbose=False,\n",
    "    #     patience=15,\n",
    "    # ),\n",
    "    OldModelCheckpoint(\n",
    "        # monitor=\"val_loss\",  <-- uncomment these lines if early stopping\n",
    "        # save_top_k=1,\n",
    "        # mode=\"min\",\n",
    "        dirpath=_outdir / \"checkpoints\",\n",
    "    ),\n",
    "]\n",
    "trainer = OldTrainer(\n",
    "    max_epochs=1,\n",
    "    logger=tensorboard_logger,\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=callbacks,\n",
    "    # REQUIRED!! to enable sobolev loss during validation\n",
    "    inference_mode=False,\n",
    ")\n",
    "trainer.fit(fastprop_model, fastprop_dataloader)  # , val_loader)  <-- include this if early stopping\n",
    "\n",
    "# the rest of this shows how to reload the best model from early stopping and make predictions\n",
    "# ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "# print(f\"Reloading best model from checkpoint file: {ckpt_path}\")\n",
    "# fastprop_model = fastprop_model.__class__.load_from_checkpoint(ckpt_path)\n",
    "# val_results = trainer.validate(mcmpnn, val_loader)\n",
    "# predictions = trainer.predict(fastprop_model, predict_dataloader)  <-- use this to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jackson/miniconda3/envs/fastsolv/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/jackson/miniconda3/envs/fastsolv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/jackson/fastsolv/paper/demo_output/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 181 K  | train\n",
      "4 | X_d_transform   | ScaleTransform     | 0      | train\n",
      "---------------------------------------------------------------\n",
      "409 K     Trainable params\n",
      "0         Non-trainable params\n",
      "409 K     Total params\n",
      "1.638     Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 12.20it/s, v_num=1, train_loss=0.708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 11.28it/s, v_num=1, train_loss=0.708]\n"
     ]
    }
   ],
   "source": [
    "tensorboard_logger = NewTensorBoardLogger(\n",
    "    _outdir,\n",
    "    name=\"tensorboard_logs\",\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "callbacks = [\n",
    "    # EarlyStopping(\n",
    "    #     monitor=\"val_loss\",\n",
    "    #     mode=\"min\",\n",
    "    #     verbose=False,\n",
    "    #     patience=15,\n",
    "    # ),\n",
    "    NewModelCheckpoint(\n",
    "        # monitor=\"val_loss\",  <-- uncomment these lines if early stopping\n",
    "        # save_top_k=1,\n",
    "        # mode=\"min\",\n",
    "        dirpath=_outdir / \"checkpoints\",\n",
    "    ),\n",
    "]\n",
    "trainer = NewTrainer(\n",
    "    max_epochs=1,\n",
    "    logger=tensorboard_logger,\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=callbacks,\n",
    "    # REQUIRED!! to enable sobolev loss during validation\n",
    "    inference_mode=False,\n",
    ")\n",
    "trainer.fit(chemprop_model, chemprop_loader)\n",
    "\n",
    "# ... same logic as above applies for loading the chemprop model and running inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastsolv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
